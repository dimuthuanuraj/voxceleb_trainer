# Research Progress Log - October 28, 2025

**Date:** October 28, 2025  
**Project:** VoxCeleb Speaker Recognition Training Optimization  
**Repository:** https://github.com/dimuthuanuraj/SL_ColvaiAI

---

## Summary

Major documentation day - created mini dataset (140 speakers) and comprehensive training guide for rapid experimentation.

---

## Changes Made

### 1. Mini VoxCeleb1 Dataset Creation
**Files:** Data organization scripts

**Dataset Statistics:**
```
Speakers: 140 (vs 5991 full dataset)
Train utterances: ~28,000 (vs ~148k full)
Test utterances: ~7,800 (vs ~4.7k full)
Size: ~3.2 GB (vs ~30 GB full)
```

**Purpose:**
- Rapid model architecture testing
- Quick parameter tuning experiments
- Faster debugging cycles
- Resource-efficient development

**Creation Process:**
1. Selected 140 speakers from VoxCeleb1
2. Preserved train/test split structure
3. Maintained data quality
4. Created separate configuration

**Data Organization:**
```
/mnt/ricproject2/voxceleb_new/mini_voxceleb1/
â”œâ”€â”€ train/
â”‚   â””â”€â”€ wav/  # 140 speaker folders with audio files
â””â”€â”€ test/
    â””â”€â”€ wav/  # Same 140 speakers, different utterances
```

### 2. Mini Dataset Configuration
**File:** `mini_voxceleb1_config.yaml`

**Key Settings:**
```yaml
train_list: /mnt/ricproject2/voxceleb_new/mini_voxceleb1_train.txt
test_list: /mnt/ricproject2/voxceleb_new/mini_voxceleb1_test.txt
train_path: /mnt/ricproject2/voxceleb_new/mini_voxceleb1/train/wav
test_path: /mnt/ricproject2/voxceleb_new/mini_voxceleb1/test/wav
nClasses: 140

# Optimized settings
batch_size: 200
max_frames: 200
nPerSpeaker: 2
max_test_pairs: 10000

# Model
model: ResNetSE34V2
nOut: 512

# Optimizer
optimizer: adam
lr: 0.001

# Training
max_epoch: 100
test_interval: 1
```

**Git Commit:**
- `c77e0a8` - "Add mini VoxCeleb1 configuration for rapid experimentation"

### 3. Comprehensive Training Guide
**File:** `MINI_VOXCELEB1_TRAINING_GUIDE.md`

**Content Sections:**

#### Dataset Overview
- Mini vs Full dataset comparison
- Use cases for mini dataset
- Speed estimates

#### Quick Start
```bash
# Basic training
python trainSpeakerNet_performance_updated.py \
  --config configs/mini_voxceleb1_config.yaml

# Fast validation
python trainSpeakerNet_performance_updated.py \
  --config configs/mini_voxceleb1_config.yaml \
  --max_test_pairs 10000

# Debug mode
python trainSpeakerNet_performance_updated.py \
  --config configs/mini_voxceleb1_config.yaml \
  --max_test_pairs 1000
```

#### Training Time Estimates
```
Full dataset (5991 speakers):
  - Epoch: ~6 min
  - Validation: ~7 min (full) / ~4 min (10k pairs)
  
Mini dataset (140 speakers):
  - Epoch: ~8 sec
  - Validation: ~2 sec (full) / ~1 sec (10k pairs)
  
Speedup: ~45x faster per epoch! ðŸš€
```

#### Parameter Tuning Guide
- `nPerSpeaker`: 1, 2, or 3 (affects batch composition)
- `max_frames`: 200 (standard), 300 (more context)
- `batch_size`: 200 (recommended), adjust based on GPU
- `max_test_pairs`: 10000 (balanced), 0 (full), 1000 (debug)

#### Model Architecture Experiments
- ResNetSE34V2 (default, 512-dim)
- ResNetSE34L (larger, 256-dim)
- ResNetSE34 (baseline)
- ECAPA_TDNN (modern alternative)

#### Learning Rate Experiments
```
Conservative: 0.0001 (stable, slower)
Recommended:  0.001  (good balance)
Aggressive:   0.01   (risk of divergence)
```

#### Advanced Configurations
- Mixed precision training
- Gradient accumulation
- Custom learning rate schedules
- Different optimizers (Adam, SGD)

#### Common Issues & Solutions
1. **NaN Loss:**
   - Reduce learning rate
   - Check gradient accumulation
   - Verify data normalization

2. **Low Accuracy:**
   - Increase max_frames
   - Adjust nPerSpeaker
   - Verify data augmentation

3. **Slow Training:**
   - Increase batch_size
   - Use max_test_pairs for validation
   - Enable mixed precision

4. **Out of Memory:**
   - Reduce batch_size
   - Reduce max_frames
   - Use gradient accumulation

#### Performance Monitoring
```bash
# Watch training
watch -n 1 tail -n 30 exps/mini_exp/train.log

# Check GPU usage
nvidia-smi

# Monitor validation
tail -f exps/mini_exp/val.log
```

#### Best Practices
- Start with mini dataset for architecture testing
- Use full dataset for final training
- Save checkpoints every 5-10 epochs
- Monitor EER trends, not individual values
- Test on full validation periodically

**Git Commit:**
- `3e5cf92` - "Add comprehensive mini VoxCeleb1 training guide"

### 4. Dataset List Files
**Files:** 
- `mini_voxceleb1_train.txt` (28,000 lines)
- `mini_voxceleb1_test.txt` (7,800 lines)

**Format:**
```
1 /mnt/ricproject2/voxceleb_new/mini_voxceleb1/train/wav/id10001/1zcIwhmdeo4/00001.wav
1 /mnt/ricproject2/voxceleb_new/mini_voxceleb1/train/wav/id10001/1zcIwhmdeo4/00002.wav
...
```

**Git Commit:**
- `8d4ac7b` - "Add train/test lists for mini VoxCeleb1 dataset"

---

## Dataset Comparison

| Metric | Mini Dataset | Full Dataset | Speedup |
|--------|-------------|--------------|---------|
| Speakers | 140 | 5,991 | 42.8x fewer |
| Train Files | ~28,000 | ~148,000 | 5.3x fewer |
| Size | 3.2 GB | 30 GB | 9.4x smaller |
| Epoch Time | ~8 sec | ~6 min | 45x faster |
| Validation | ~2 sec | ~7 min | 210x faster |
| Full Train (100 epochs) | ~13 min | ~10 hours | 46x faster |

---

## Use Cases

### Mini Dataset Perfect For:
âœ… Architecture experiments  
âœ… Parameter tuning  
âœ… Learning rate testing  
âœ… Quick debugging  
âœ… Feature prototyping  
âœ… Code validation  

### Full Dataset Required For:
âœ… Final model training  
âœ… Performance benchmarking  
âœ… Publication results  
âœ… Production models  

---

## Git Commits Summary

**Total Commits Today:** 3

1. `c77e0a8` - Add mini VoxCeleb1 configuration
2. `3e5cf92` - Add comprehensive training guide
3. `8d4ac7b` - Add train/test lists

---

## Training Quick Reference

```bash
# Mini dataset (fast experimentation)
python trainSpeakerNet_performance_updated.py \
  --config configs/mini_voxceleb1_config.yaml

# Full dataset (production training)
python trainSpeakerNet_performance_updated.py \
  --config configs/BaselineConfig.yaml

# Resume training
python trainSpeakerNet_performance_updated.py \
  --config configs/mini_voxceleb1_config.yaml \
  --initial_model exps/mini_exp/model/model000000020.model
```

---

## Documentation Files Created

1. **mini_voxceleb1_config.yaml** - Training configuration
2. **MINI_VOXCELEB1_TRAINING_GUIDE.md** - 400+ line comprehensive guide
3. **mini_voxceleb1_train.txt** - Training file list
4. **mini_voxceleb1_test.txt** - Testing file list

---

## Time Investment

- Dataset creation: 4 hours
- Configuration setup: 2 hours
- Documentation writing: 4 hours
- Testing: 2 hours

**Total: ~12 hours**

---

**End of Report - October 28, 2025**

**Achievement: Mini dataset enables 45x faster experimentation! ðŸš€**
