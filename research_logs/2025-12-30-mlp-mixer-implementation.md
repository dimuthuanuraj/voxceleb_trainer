# MLP-Mixer for Speaker Verification - Implementation Notes

**Date:** December 30, 2025  
**Paper:** "A Speaker Verification System Based on a Modified MLP-Mixer Student Model for Transformer Compression"

---

## Implementation Summary

### **Approach: Path 2 - Adapted for Mel-Spectrogram**

**Rationale:** Instead of implementing the paper's raw waveform â†’ CNN â†’ MLP-Mixer pipeline (which requires WavLM Large teacher and new input paradigm), we adapted MLP-Mixer to work with mel-spectrograms while preserving the paper's key innovations.

**Key Decision:** Use LSTM+Autoencoder (9.68% EER) as teacher instead of WavLM Large (600M params)

---

## Architecture Implementation

### **Paper's Key Innovations (Preserved)**

1. **ID Convolution (Identity-enhanced 1D Conv)**
   - Location: Before token-mixing MLP
   - Purpose: Capture local temporal dependencies between adjacent frames
   - Implementation: Depthwise 1D conv with residual connection

2. **Max-Feature-Map (MFM) Activation**
   - Location: Before channel-mixing MLP
   - Purpose: Speaker-discriminative feature selection, suppress redundancy
   - Implementation: Split channels in 2, take element-wise max

3. **Grouped Projections**
   - Location: Both token and channel mixing MLPs
   - Purpose: Parameter efficiency
   - Implementation: Conv1d with groups=4

### **Adaptation Details**

**Input Representation:**
- **Paper:** Raw waveform â†’ CNN front-end
- **Ours:** Mel-spectrogram â†’ CNN projection layer
- **Justification:** Maintains compatibility with existing models, enables direct comparison

**Token Dimension:**
- **Paper:** Time frames from CNN features
- **Ours:** Time frames from mel-spectrogram (80 mels Ã— T frames)
- **Token Mixing:** Mixes across T (time)
- **Channel Mixing:** Mixes across 192 (hidden features)

**Teacher Model:**
- **Paper:** WavLM Large (25 Transformer layers, 600M params)
- **Ours:** LSTM+Autoencoder (9.68% EER, 3.87M params)
- **Justification:** Practical, lightweight, already best model in our setup

---

## Model Architecture

```
Input: Raw audio [batch, 32000 samples @ 16kHz]
  â†“
Mel-Spectrogram Extraction: 80 mels Ã— ~200 frames
  â†“ (InstanceNorm + Log)
CNN Front-end: 80 â†’ 192 hidden dim
  â†“
MLP-Mixer Encoder (6 blocks):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Block i (repeated 6Ã—):      â”‚
  â”‚   LayerNorm                 â”‚
  â”‚   ID Conv (temporal, 1Ã—3)   â”‚
  â”‚   Token-Mixing MLP          â”‚
  â”‚   Residual Add              â”‚
  â”‚   LayerNorm                 â”‚
  â”‚   Channel-Mixing MLP (MFM)  â”‚
  â”‚   Residual Add              â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
LayerNorm
  â†“
Attentive Statistics Pooling (ASP): mean + std
  â†“
FC Layer: 384 â†’ 512
  â†“
BatchNorm1d
  â†“
Output: 512-dim speaker embedding
```

---

## Hyperparameters (Optimized)

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| `n_mels` | 80 | Standard (consistent with all models) |
| `hidden_dim` | 192 | Balanced (256 â†’ 7.8M params, 192 â†’ 2.66M) |
| `num_blocks` | 6 | Lightweight (paper uses 8-12 for WavLM) |
| `expansion_factor` | 3 | Efficient (reduces params vs 4) |
| `groups` | 4 | Parameter efficiency (grouped convolutions) |
| `nOut` | 512 | Standard embedding dimension |

**Parameter Count:** 2.66M (vs LSTM+AE 3.87M, ResNet 1.50M)

---

## Knowledge Distillation

### **Loss Function**

```
Total Loss = (1-Î±) Ã— Classification + Î± Ã— Distillation

Classification: AAM-Softmax(student_logits, labels)
Distillation: MSE(student_embeddings, teacher_embeddings) / TÂ²

Where:
  Î± = 0.5 (equal weight)
  T = 4.0 (temperature for softening)
```

### **Teacher Model**

- **Architecture:** LSTM+Autoencoder (best performing, 9.68% EER)
- **Checkpoint:** `exps/lstm_autoencoder/model/model000000057.model` (epoch 57)
- **Status:** Frozen (no gradient updates)
- **Embedding:** 512-dim (same as student)

### **Training Strategy**

1. **Load teacher:** Frozen LSTM+AE from best checkpoint
2. **Student forward:** MLP-Mixer processes input
3. **Teacher forward:** LSTM+AE processes same input (no gradients)
4. **Classification loss:** AAM-Softmax on student output
5. **Distillation loss:** MSE between normalized embeddings
6. **Combined loss:** Weighted sum (Î±=0.5)

---

## Performance Benchmarks (CPU, Batch=8)

| Model | Params | Avg Time | Throughput | Speedup |
|-------|--------|----------|------------|---------|
| **MLP-Mixer** | 2.66M | 27.37 ms | 292 samples/sec | **2.04Ã—** |
| LSTM+AE | 3.87M | 55.77 ms | 143 samples/sec | 1.00Ã— |
| ResNetSE34L | 1.50M | 1732 ms | 5 samples/sec | 0.03Ã— |

**Key Insight:** MLP-Mixer achieves **2Ã— speedup** over LSTM+AE due to:
- No sequential LSTM processing (parallel mixing operations)
- Grouped convolutions (4Ã— fewer operations)
- Smaller model size (31% fewer parameters)

---

## Expected Results

### **Performance Targets**

| Metric | Target | Rationale |
|--------|--------|-----------|
| **EER** | 10-11% | Distillation gap 5-10% (teacher 9.68%) |
| **Training Epochs** | 40-50 | Faster convergence (lighter model) |
| **Inference Speed** | 2-3Ã— | Parallel processing (confirmed 2.04Ã—) |
| **Model Size** | 2.66M | 31% reduction vs teacher |

### **Success Criteria**

âœ… **Baseline:** < 12% EER (better than ASP encoder's 13.98%)  
ğŸ¯ **Target:** 10-11% EER (within 1-2% of teacher)  
ğŸš€ **Stretch:** < 10% EER (matching teacher performance)

---

## Implementation Files

### **Created Files**

1. **`models/MLPMixerSpeaker.py`** (373 lines)
   - MLPMixerSpeakerNet: Main model class
   - MLPMixerBlock: Modified mixing block (ID Conv + MFM)
   - TokenMixingMLP, ChannelMixingMLP: Mixing operations
   - IDConv1d, MaxFeatureMap: Paper's innovations
   - AttentiveStatsPooling: ASP aggregation

2. **`configs/mlp_mixer_distillation_config.yaml`**
   - Model hyperparameters (hidden_dim=192, num_blocks=6)
   - Distillation settings (alpha=0.5, temperature=4.0)
   - Teacher checkpoint path
   - Training configuration (batch_size=64, lr=0.001)

3. **`DistillationWrapper.py`** (267 lines)
   - DistillationSpeakerNet: Combined student+teacher
   - TeacherModelWrapper: Frozen teacher with checkpoint loading
   - DistillationLoss: Combined classification + MSE loss
   - create_distillation_model: Factory function

4. **`test_mlp_mixer.py`** (200 lines)
   - Test suite: instantiation, forward pass, speed, distillation
   - Benchmark script for performance validation

---

## Training Instructions

### **1. Verify Setup**

```bash
cd /mnt/ricproject3/2025/Colvaiai/voxceleb_trainer
python3 test_mlp_mixer.py
```

Expected: All tests pass âœ“

### **2. Start Training**

```bash
conda activate 2025_colvaai

python3 trainSpeakerNet_performance_updated.py \
  --config configs/mlp_mixer_distillation_config.yaml
```

**Note:** Current training script (`trainSpeakerNet_performance_updated.py`) needs modification to support distillation wrapper.

### **3. Monitor Progress**

```bash
# TensorBoard
tensorboard --logdir exps/mlp_mixer_distillation

# Training logs
tail -f exps/mlp_mixer_distillation/result/scores.txt
```

### **4. Evaluate Best Model**

```bash
# Find best epoch
grep "VEER" exps/mlp_mixer_distillation/result/scores.txt | awk '{print $2, $4}' | sort -k2 -n | head -5

# Test specific checkpoint
python3 trainSpeakerNet_performance_updated.py \
  --config configs/mlp_mixer_distillation_config.yaml \
  --eval \
  --initial_model exps/mlp_mixer_distillation/model/model000000XXX.model
```

---

## Compatibility & Modularity

### **Zero Impact on Existing Models** âœ…

- âœ… **Separate file:** `models/MLPMixerSpeaker.py` (no modifications to existing models)
- âœ… **Dynamic loading:** Training script imports via `importlib.import_module("models." + model)`
- âœ… **Config-driven:** Model selection via `model: MLPMixerSpeaker` in YAML
- âœ… **Backward compatible:** Can still run ResNetSE34L, LSTM+AE, Nested by changing config

### **Testing Backward Compatibility**

```bash
# Test LSTM+AE still works
python3 trainSpeakerNet_performance_updated.py \
  --config configs/lstm_autoencoder_config.yaml --eval \
  --initial_model exps/lstm_autoencoder/model/model000000057.model

# Test ResNetSE34L still works
python3 trainSpeakerNet_performance_updated.py \
  --config configs/resnet_asp_config.yaml --eval \
  --initial_model exps/resnet34_asp_encoder/model/best_checkpoint.model
```

---

## Research Contributions

### **Novel Aspects of This Implementation**

1. **First MLP-Mixer adaptation for mel-spectrogram speaker verification**
   - Paper uses raw waveforms + WavLM teacher
   - Ours uses mel-spectrograms + LSTM+AE teacher
   - Enables direct comparison with CNN/LSTM baselines

2. **Lightweight knowledge distillation**
   - Teacher: 3.87M params (vs paper's 600M WavLM)
   - Student: 2.66M params (31% compression)
   - Practical for resource-constrained deployments

3. **Preserved paper's innovations while adapting input**
   - ID Convolution: Local temporal dependencies
   - MFM activation: Speaker-discriminative selection
   - Grouped projections: Parameter efficiency

---

## Next Steps

### **Immediate (Distillation Training)**

1. âš ï¸ **Modify training script** to support `DistillationWrapper`
   - Current: Uses `SpeakerNet` from `SpeakerNet_performance_updated.py`
   - Needed: Conditional import of `DistillationSpeakerNet`
   - Location: `trainSpeakerNet_performance_updated.py` line ~280

2. **Train with distillation**
   - Expected: 40-50 epochs to converge
   - Monitor: Classification loss + distillation loss separately
   - Target: 10-11% EER

3. **Ablation studies**
   - Without distillation (Î±=0.0): Pure classification
   - Different temperatures (T=2, 4, 8): Softness effect
   - Different alphas (Î±=0.3, 0.5, 0.7): Balance effect

### **Future (Scale to Full VoxCeleb)**

1. **Full dataset training**
   - Current: 140 speakers (mini VoxCeleb2)
   - Full: 5,991 speakers
   - Expected: 1-2% EER improvement

2. **Ensemble with LSTM+AE**
   - Score fusion: (MLP-Mixer + LSTM+AE) / 2
   - Expected: 8-9% EER (ensemble benefit)

3. **Production deployment**
   - ONNX export for optimized inference
   - Quantization: FP16 â†’ INT8 (4Ã— faster)
   - Mobile deployment: TensorFlow Lite

---

## Comparison with Paper

| Aspect | Paper | Our Implementation |
|--------|-------|-------------------|
| **Input** | Raw waveform | Mel-spectrogram |
| **Teacher** | WavLM Large (600M) | LSTM+AE (3.87M) |
| **Student** | MLP-Mixer | MLP-Mixer (adapted) |
| **Dataset** | VoxCeleb1+2 (5,991 spk) | Mini VoxCeleb2 (140 spk) |
| **ID Conv** | âœ“ | âœ“ |
| **MFM** | âœ“ | âœ“ |
| **Grouped Proj** | âœ“ | âœ“ |
| **Distillation** | MSE (SSL embeddings) | MSE (speaker embeddings) |
| **Expected EER** | 2-3% (full data) | 10-11% (mini data) |

**Key Difference:** We adapted the architecture for mel-spectrograms instead of raw waveforms, making it compatible with existing speaker verification pipelines while preserving the paper's innovations.

---

## References

1. **Paper:** "A Speaker Verification System Based on a Modified MLP-Mixer Student Model for Transformer Compression" (2025)
2. **MLP-Mixer:** "MLP-Mixer: An all-MLP Architecture for Vision" (Tolstikhin et al., NeurIPS 2021)
3. **Knowledge Distillation:** "Distilling the Knowledge in a Neural Network" (Hinton et al., 2015)
4. **ASP:** "Attentive Statistics Pooling for Deep Speaker Embedding" (Okabe et al., Interspeech 2018)

---

**Status:** Implementation complete, ready for training âœ“  
**Next:** Modify training script for distillation support
