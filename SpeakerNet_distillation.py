#!/usr/bin/python
# -*- coding: utf-8 -*-

"""
DISTILLATION-AWARE SPEAKERNET - SpeakerNet_distillation.py
Based on SpeakerNet_performance_updated.py with knowledge distillation support

NEW: Automatically uses DistillationSpeakerNet when teacher_model is specified
- Detects teacher parameters in config (teacher_model, teacher_checkpoint)
- Creates DistillationSpeakerNet for student-teacher training
- Falls back to standard SpeakerNet for backward compatibility
- Zero impact on existing models (separate script)

Optimizations applied:
1. Gradient accumulation support
2. Improved mixed precision training
3. Memory optimization with gradient checkpointing
4. Better tensor operations
5. Optimized evaluation with batching
6. Reduced CPU-GPU synchronization
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy, sys, random
import time, itertools, importlib

from DatasetLoader import test_dataset_loader
from torch.cuda.amp import autocast, GradScaler

# Import distillation wrapper
from DistillationWrapper import DistillationSpeakerNet


class WrappedModel(nn.Module):

    ## The purpose of this wrapper is to make the model structure consistent between single and multi-GPU

    def __init__(self, model):
        super(WrappedModel, self).__init__()
        self.module = model

    def forward(self, x, label=None):
        return self.module(x, label)


class SpeakerNet(nn.Module):
    """
    Standard SpeakerNet or Distillation-aware wrapper
    
    Automatically detects if distillation should be used based on kwargs:
    - If teacher_model and teacher_checkpoint exist â†’ use DistillationSpeakerNet
    - Otherwise â†’ use standard SpeakerNet
    """
    def __init__(self, model, optimizer, trainfunc, nPerSpeaker, **kwargs):
        super(SpeakerNet, self).__init__()

        # Check if distillation parameters are provided
        teacher_model = kwargs.get('teacher_model', None)
        teacher_checkpoint = kwargs.get('teacher_checkpoint', None)
        
        if teacher_model and teacher_checkpoint:
            # Use distillation wrapper
            print('\n' + '='*60)
            print('ðŸŽ“ DISTILLATION MODE ENABLED')
            print('='*60)
            self.__impl__ = DistillationSpeakerNet(
                student_model=model,
                teacher_model=teacher_model,
                teacher_checkpoint=teacher_checkpoint,
                optimizer=optimizer,
                trainfunc=trainfunc,
                nPerSpeaker=nPerSpeaker,
                distillation_alpha=kwargs.get('distillation_alpha', 0.5),
                distillation_temperature=kwargs.get('distillation_temperature', 4.0),
                freeze_teacher=kwargs.get('freeze_teacher', True),
                **kwargs
            )
            self.use_distillation = True
        else:
            # Use standard SpeakerNet (backward compatibility)
            print('\nðŸ“š STANDARD CLASSIFICATION MODE')
            SpeakerNetModel = importlib.import_module("models." + model).__getattribute__("MainModel")
            self.__S__ = SpeakerNetModel(**kwargs)

            LossFunction = importlib.import_module("loss." + trainfunc).__getattribute__("LossFunction")
            self.__L__ = LossFunction(**kwargs)

            self.nPerSpeaker = nPerSpeaker
            self.use_distillation = False

    def forward(self, data, label=None):
        """
        Forward pass - delegates to distillation or standard implementation
        
        Returns:
        - Distillation mode: (total_loss, accuracy, distillation_loss)
        - Standard mode: (loss, accuracy) or embeddings
        """
        if self.use_distillation:
            return self.__impl__(data, label)
        else:
            # Standard SpeakerNet forward pass
            # OPTIMIZATION: Avoid unnecessary reshape if already correct shape
            if data.dim() == 3:
                data = data.reshape(-1, data.size()[-1])
            
            data = data.cuda(non_blocking=True)  # OPTIMIZATION: Non-blocking transfer
            outp = self.__S__.forward(data)

            if label == None:
                return outp

            else:
                outp = outp.reshape(self.nPerSpeaker, -1, outp.size()[-1]).transpose(1, 0).squeeze(1)
                nloss, prec1 = self.__L__.forward(outp, label)
                return nloss, prec1


class ModelTrainer(object):
    def __init__(self, speaker_model, optimizer, scheduler, gpu, mixedprec, **kwargs):

        self.__model__ = speaker_model

        Optimizer = importlib.import_module("optimizer." + optimizer).__getattribute__("Optimizer")
        self.__optimizer__ = Optimizer(self.__model__.parameters(), **kwargs)

        Scheduler = importlib.import_module("scheduler." + scheduler).__getattribute__("Scheduler")
        self.__scheduler__, self.lr_step = Scheduler(self.__optimizer__, **kwargs)

        # OPTIMIZATION: Initialize GradScaler with better defaults
        self.scaler = GradScaler(
            init_scale=2.**16,
            growth_factor=2.0,
            backoff_factor=0.5,
            growth_interval=2000,
            enabled=mixedprec
        )

        self.gpu = gpu

        self.mixedprec = mixedprec

        assert self.lr_step in ["epoch", "iteration"]

    ## ===== ===== ===== ===== ===== ===== ===== =====
    ## Train network - OPTIMIZED
    ## ===== ===== ===== ===== ===== ===== ===== =====

    def train_network(self, loader, verbose, gradient_accumulation_steps=1):

        self.__model__.train()

        stepsize = loader.batch_size

        counter = 0
        index = 0
        loss = 0
        top1 = 0
        distill_loss_accum = 0  # For distillation loss tracking

        tstart = time.time()

        # OPTIMIZATION: Use enumerate for better iteration
        for batch_idx, (data, data_label) in enumerate(loader):

            data = data.transpose(1, 0)

            # OPTIMIZATION: Move label to GPU with non_blocking
            label = torch.LongTensor(data_label).cuda(non_blocking=True)

            # OPTIMIZATION: Mixed precision with gradient accumulation
            if self.mixedprec:
                with autocast():
                    model_output = self.__model__(data, label)
                
                # Handle distillation output (3 values) vs standard output (2 values)
                if len(model_output) == 3:
                    nloss, prec1, distill_loss = model_output
                    distill_loss_accum += distill_loss
                else:
                    nloss, prec1 = model_output
                
                # Scale loss by accumulation steps
                nloss = nloss / gradient_accumulation_steps
                
                self.scaler.scale(nloss).backward()
                
                # Only step optimizer every N iterations
                if (batch_idx + 1) % gradient_accumulation_steps == 0:
                    # OPTIMIZATION: Gradient clipping for stability
                    self.scaler.unscale_(self.__optimizer__)
                    torch.nn.utils.clip_grad_norm_(self.__model__.parameters(), max_norm=5.0)
                    
                    self.scaler.step(self.__optimizer__)
                    self.scaler.update()
                    self.__optimizer__.zero_grad(set_to_none=True)  # OPTIMIZATION: set_to_none=True for better memory
            else:
                model_output = self.__model__(data, label)
                
                # Handle distillation output (3 values) vs standard output (2 values)
                if len(model_output) == 3:
                    nloss, prec1, distill_loss = model_output
                    distill_loss_accum += distill_loss
                else:
                    nloss, prec1 = model_output
                
                nloss = nloss / gradient_accumulation_steps
                nloss.backward()
                
                if (batch_idx + 1) % gradient_accumulation_steps == 0:
                    torch.nn.utils.clip_grad_norm_(self.__model__.parameters(), max_norm=5.0)
                    self.__optimizer__.step()
                    self.__optimizer__.zero_grad(set_to_none=True)

            # OPTIMIZATION: Accumulate losses correctly
            loss += nloss.detach().cpu().item() * gradient_accumulation_steps
            top1 += prec1.detach().cpu().item()
            counter += 1
            index += stepsize

            telapsed = time.time() - tstart
            tstart = time.time()

            if verbose:
                sys.stdout.write("\rProcessing {:d} of {:d}: ".format(index, loader.__len__() * loader.batch_size))
                sys.stdout.write("Loss {:f} TEER/TAcc {:2.3f}% - {:.2f} Hz ".format(loss / counter, top1 / counter, stepsize / telapsed))
                sys.stdout.flush()

            if self.lr_step == "iteration" and (batch_idx + 1) % gradient_accumulation_steps == 0:
                self.__scheduler__.step()

        if self.lr_step == "epoch":
            self.__scheduler__.step()

        # Return distillation loss if it was accumulated
        if distill_loss_accum > 0:
            return (loss / counter, top1 / counter, distill_loss_accum / counter)
        else:
            return (loss / counter, top1 / counter)

    ## ===== ===== ===== ===== ===== ===== ===== =====
    ## Evaluate from list - OPTIMIZED
    ## ===== ===== ===== ===== ===== ===== ===== =====

    def evaluateFromList(self, test_list, test_path, nDataLoaderThread, distributed, print_interval=100, num_eval=10, max_test_pairs=0, **kwargs):

        if distributed:
            rank = torch.distributed.get_rank()
        else:
            rank = 0

        self.__model__.eval()

        lines = []
        files = []
        feats = {}
        tstart = time.time()

        ## Read all lines
        with open(test_list) as f:
            lines = f.readlines()

        ## Limit test pairs if max_test_pairs is specified (NEW FEATURE)
        if max_test_pairs > 0 and max_test_pairs < len(lines):
            if rank == 0:
                print(f"âš ï¸  Using first {max_test_pairs} test pairs out of {len(lines)} total pairs")
            lines = lines[:max_test_pairs]

        ## Get a list of unique file names
        files = list(itertools.chain(*[x.strip().split()[-2:] for x in lines]))
        setfiles = list(set(files))
        setfiles.sort()

        ## Define test data loader - OPTIMIZED
        test_dataset = test_dataset_loader(setfiles, test_path, num_eval=num_eval, **kwargs)

        if distributed:
            sampler = torch.utils.data.distributed.DistributedSampler(test_dataset, shuffle=False)
        else:
            sampler = None

        # OPTIMIZATION: Larger batch size for faster evaluation on GPU
        eval_batch_size = kwargs.get('eval_batch_size', 32)  # NEW: Configurable eval batch size
        
        test_loader = torch.utils.data.DataLoader(
            test_dataset, 
            batch_size=eval_batch_size,  # OPTIMIZATION: Increased from 1 to 32 (or configurable)
            shuffle=False, 
            num_workers=nDataLoaderThread, 
            drop_last=False, 
            sampler=sampler,
            pin_memory=True,  # OPTIMIZATION: Faster CPU->GPU
            prefetch_factor=3  # OPTIMIZATION: Increased prefetch for evaluation
        )

        ## Extract features for every batch
        for idx, data in enumerate(test_loader):
            inp1 = data[0].cuda(non_blocking=True)  # OPTIMIZATION: Non-blocking transfer (batch now)
            
            # OPTIMIZATION: Use inference mode instead of no_grad for better performance
            with torch.inference_mode():
                ref_feat = self.__model__(inp1).detach().cpu()
            
            # Store features for each file in the batch
            for batch_idx in range(ref_feat.size(0)):
                file_idx = idx * eval_batch_size + batch_idx
                if file_idx < len(setfiles):
                    feats[setfiles[file_idx]] = ref_feat[batch_idx]
            
            telapsed = time.time() - tstart

            if idx % print_interval == 0 and rank == 0:
                sys.stdout.write(
                    "\rReading {:d} of {:d}: {:.2f} Hz, embedding size {:d}".format(
                        idx * eval_batch_size, len(setfiles), (idx * eval_batch_size) / telapsed, ref_feat.size()[1]
                    )
                )

        all_scores = []
        all_labels = []
        all_trials = []

        if distributed:
            ## Gather features from all GPUs
            feats_all = [None for _ in range(0, torch.distributed.get_world_size())]
            torch.distributed.all_gather_object(feats_all, feats)

        if rank == 0:

            tstart = time.time()
            print("")

            ## Combine gathered features
            if distributed:
                feats = feats_all[0]
                for feats_batch in feats_all[1:]:
                    feats.update(feats_batch)

            ## Read files and compute all scores - OPTIMIZED
            for idx, line in enumerate(lines):

                data = line.split()

                ## Append random label if missing
                if len(data) == 2:
                    data = [random.randint(0, 1)] + data

                ref_feat = feats[data[1]].cuda(non_blocking=True)  # OPTIMIZATION: Non-blocking
                com_feat = feats[data[2]].cuda(non_blocking=True)

                # OPTIMIZATION: Use inference mode
                with torch.inference_mode():
                    if self.__model__.module.__L__.test_normalize:
                        # Normalize - features are already 1D vectors from batch processing
                        ref_feat = F.normalize(ref_feat.unsqueeze(0), p=2, dim=1).squeeze(0)
                        com_feat = F.normalize(com_feat.unsqueeze(0), p=2, dim=1).squeeze(0)

                    # Compute distance - expand to proper shape for cdist
                    dist = torch.cdist(
                        ref_feat.unsqueeze(0).unsqueeze(0), 
                        com_feat.unsqueeze(0).unsqueeze(0)
                    ).detach().cpu().numpy()

                score = -1 * numpy.mean(dist)

                all_scores.append(score)
                all_labels.append(int(data[0]))
                all_trials.append(data[1] + " " + data[2])

                if idx % print_interval == 0:
                    telapsed = time.time() - tstart
                    sys.stdout.write("\rComputing {:d} of {:d}: {:.2f} Hz".format(idx, len(lines), idx / telapsed))
                    sys.stdout.flush()

            # OPTIMIZATION: Clear GPU cache after evaluation
            torch.cuda.empty_cache()

        return (all_scores, all_labels, all_trials)

    ## ===== ===== ===== ===== ===== ===== ===== =====
    ## Save parameters
    ## ===== ===== ===== ===== ===== ===== ===== =====

    def saveParameters(self, path):

        torch.save(self.__model__.module.state_dict(), path)

    ## ===== ===== ===== ===== ===== ===== ===== =====
    ## Load parameters
    ## ===== ===== ===== ===== ===== ===== ===== =====

    def loadParameters(self, path):

        self_state = self.__model__.module.state_dict()
        loaded_state = torch.load(path, map_location="cuda:%d" % self.gpu)
        
        if len(loaded_state.keys()) == 1 and "model" in loaded_state:
            loaded_state = loaded_state["model"]
            newdict = {}
            delete_list = []
            for name, param in loaded_state.items():
                new_name = "__S__."+name
                newdict[new_name] = param
                delete_list.append(name)
            loaded_state.update(newdict)
            for name in delete_list:
                del loaded_state[name]
        
        for name, param in loaded_state.items():
            origname = name
            if name not in self_state:
                name = name.replace("module.", "")

                if name not in self_state:
                    print("{} is not in the model.".format(origname))
                    continue

            if self_state[name].size() != loaded_state[origname].size():
                print("Wrong parameter length: {}, model: {}, loaded: {}".format(
                    origname, self_state[name].size(), loaded_state[origname].size()
                ))
                continue

            self_state[name].copy_(param)
