# FEW-SHOT CONFIG - GE2E Loss for Speaker Verification
# Experiment: Few-shot learning with Generalized End-to-End (GE2E) loss
# Best for: Small datasets, quick speaker adaptation, enrollment scenarios

## Data loader
augment: true
batch_size: 32  # 32 speakers per batch
max_frames: 200
eval_frames: 0
nDataLoaderThread: 4
max_seg_per_spk: 500
seed: 42

## Training details
test_interval: 1
trainfunc: ge2e  # CHANGED: Few-shot loss (was aamsoftmax)
patience: 15
max_epoch: 100

## Optimizer
optimizer: adam
scheduler: steplr
lr: 0.001
lr_decay: 0.95
weight_decay: 0.0001

## Loss functions - GE2E specific
init_w: 10.0  # GE2E scaling parameter (learnable)
init_b: -5.0  # GE2E bias parameter (learnable)
nPerSpeaker: 3  # IMPORTANT: GE2E requires â‰¥3 utterances per speaker
nClasses: 140  # Not used by GE2E but keep for compatibility

## Training and test data
train_list: /mnt/ricproject3/2025/data/mini_voxceleb2_train_list.txt
test_list: /mnt/ricproject3/2025/data/mini_test_list.txt
train_path: /mnt/ricproject3/2025/data/mini_voxceleb2
test_path: /mnt/ricproject3/2025/data/mini_voxceleb1

max_test_pairs: 0
eval_batch_size: 1

# Augmentation paths
musan_path: /mnt/ricproject3/2025/data/musan
rir_path: /mnt/ricproject3/2025/data/RIRS_NOISES/simulated_rirs

## Model definition
n_mels: 80
model: ResNetSE34L
encoder_type: ASP  # Attentive Statistics Pooling (best for GE2E)
nOut: 512
log_input: true

## Load and save paths
save_path: exps/mini_voxceleb1_fewshot_ge2e
initial_model: ""

## Performance optimization
mixedprec: true
prefetch_factor: 2
persistent_workers: true
gradient_accumulation_steps: 1

## Distributed training
distributed: false
port: "8888"

## Advanced optimization
compile_model: false
enable_profiling: false
